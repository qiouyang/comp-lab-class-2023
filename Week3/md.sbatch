#!/bin/bash

#SBATCH --nodes=20
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --time=24:00:00
#SBATCH --mem=8GB
#SBATCH --job-name=qiouyang_assignment
#SBATCH --output=qiouyang_assignment.out

module load gromacs/openmpi/intel/2020.4

# Generated by CHARMM-GUI (http://www.charmm-gui.org) v3.7
#
# This folder contains GROMACS formatted CHARMM36 force fields, a pre-optimized PDB structure, and GROMACS inputs.
# All input files were optimized for GROMACS 2019.2 or above, so lower version of GROMACS can cause some errors.
# We adopted the Verlet cut-off scheme for all minimization, equilibration, and production steps because it is 
# faster and more accurate than the group scheme. If you have a trouble with a performance of Verlet scheme while 
# running parallelized simulation, you should check if you are using appropriate command line.
# For MPI parallelizing, we recommand following command:
# mpirun -np $NUM_CPU mpirun gmx_mpi mdrun -ntomp 1

# Production

mpirun -np 1 gmx_mpi grompp -f Setup/md.mdp -o Data/1aki_md_50ns.tpr -c Data/1aki_npt.gro -p topol.top
mpirun gmx_mpi mdrun -v -deffnm Data/1aki_md_50ns

